{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7069d33",
   "metadata": {},
   "source": [
    "# Import and preprocessing Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b680c5af",
   "metadata": {},
   "source": [
    "we will Make sentiment analysis model for imdb reviews data that can be found in tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a630331c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Osama Ismael\\Anaconda3\\envs\\env1\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras import layers\n",
    "import string\n",
    "from nltk.tokenize import sent_tokenize , word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bce974c",
   "metadata": {},
   "source": [
    "First we Download the data from the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b02af188",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_train, ds_info = tfds.load(name=\"imdb_reviews\",split=\"train\",with_info=True, as_supervised=True)\n",
    "imdb_test = tfds.load(name=\"imdb_reviews\", split=\"test\", as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a82bad1",
   "metadata": {},
   "source": [
    "we then try to encode and tokenize data so we can input it to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "593dbb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words = np.array([r.numpy().decode('utf-8') for r, l in imdb_train])\n",
    "train_lbl = [l for r, l in imdb_train]\n",
    "train_lbl = np.array(train_lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4248b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_words = np.array([r.numpy().decode('utf-8') for r, l in imdb_test])\n",
    "test_lbl = [l for r, l in imdb_test]\n",
    "test_lbl = np.array(test_lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79d71148",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_words)\n",
    "tokenized_train = tokenizer.texts_to_sequences(train_words)\n",
    "sentences = pad_sequences(tokenized_train,padding=\"post\", maxlen=150)\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = {value:key for key,value in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "615d1ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize and padding test data\n",
    "tokenized_test = tokenizer.texts_to_sequences(test_words)\n",
    "sentences_test = pad_sequences(tokenized_test,padding=\"post\", maxlen=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0b559b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((sentences,train_lbl))\n",
    "test_data  = tf.data.Dataset.from_tensor_slices((sentences_test,test_lbl))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a0e177",
   "metadata": {},
   "source": [
    "# Model without pretrained Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c3c50e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(word2idx)\n",
    "# Number of RNN units\n",
    "rnn_units = 64\n",
    "#embedding diminsion\n",
    "embedding_dim = 64\n",
    "#batch size\n",
    "BATCH_SIZE=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a474225c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_bilstm(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "    layers.Embedding(vocab_size, embedding_dim),\n",
    "    layers.Bidirectional(layers.LSTM(rnn_units, return_sequences=True,dropout=0.5)),\n",
    "    layers.Bidirectional(layers.LSTM(rnn_units, dropout=0.25)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95770dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 64)          5669248   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, None, 128)         66048     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 5,834,241\n",
      "Trainable params: 5,834,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bilstm = build_model_bilstm(vocab_size = vocab_size,embedding_dim=embedding_dim,rnn_units=rnn_units,batch_size=BATCH_SIZE)\n",
    "bilstm.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy', 'Precision', 'Recall'])\n",
    "bilstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8696f8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "250/250 [==============================] - 19s 47ms/step - loss: 0.4172 - accuracy: 0.7958 - precision: 0.7858 - recall: 0.8134\n",
      "Epoch 2/10\n",
      "250/250 [==============================] - 12s 46ms/step - loss: 0.1883 - accuracy: 0.9293 - precision: 0.9298 - recall: 0.9286\n",
      "Epoch 3/10\n",
      "250/250 [==============================] - 12s 47ms/step - loss: 0.1126 - accuracy: 0.9611 - precision: 0.9609 - recall: 0.9613\n",
      "Epoch 4/10\n",
      "250/250 [==============================] - 12s 47ms/step - loss: 0.0908 - accuracy: 0.9693 - precision: 0.9696 - recall: 0.9690\n",
      "Epoch 5/10\n",
      "250/250 [==============================] - 12s 47ms/step - loss: 0.0778 - accuracy: 0.9745 - precision: 0.9756 - recall: 0.9733\n",
      "Epoch 6/10\n",
      "250/250 [==============================] - 12s 47ms/step - loss: 0.0772 - accuracy: 0.9738 - precision: 0.9726 - recall: 0.9750\n",
      "Epoch 7/10\n",
      "250/250 [==============================] - 12s 46ms/step - loss: 0.0479 - accuracy: 0.9836 - precision: 0.9832 - recall: 0.9840\n",
      "Epoch 8/10\n",
      "250/250 [==============================] - 12s 48ms/step - loss: 0.0336 - accuracy: 0.9892 - precision: 0.9895 - recall: 0.9890\n",
      "Epoch 9/10\n",
      "250/250 [==============================] - 12s 47ms/step - loss: 0.0257 - accuracy: 0.9920 - precision: 0.9922 - recall: 0.9918\n",
      "Epoch 10/10\n",
      "250/250 [==============================] - 12s 46ms/step - loss: 0.0259 - accuracy: 0.9920 - precision: 0.9923 - recall: 0.9917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c1d7f6d7f0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_train_batched = train_data.batch(BATCH_SIZE).prefetch(100)\n",
    "bilstm.fit(encoded_train_batched, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c2986fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 6s 19ms/step - loss: 0.7267 - accuracy: 0.8427 - precision: 0.8410 - recall: 0.8453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7266671061515808,\n",
       " 0.8427199721336365,\n",
       " 0.8409742116928101,\n",
       " 0.8452799916267395]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bilstm.evaluate(test_data.batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2cea7c",
   "metadata": {},
   "source": [
    "# Importing Glove Embedding Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e652f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary Size:  400000\n"
     ]
    }
   ],
   "source": [
    "dict_w2v = {}\n",
    "with open('D:\\TensorFlow\\Trying\\glove.6B/glove.6B.50d.txt', \"r\",encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        tokens = line.split()\n",
    "        word = tokens[0]\n",
    "        vector = np.array(tokens[1:], dtype=np.float32)\n",
    "        if vector.shape[0] == 50:\n",
    "            dict_w2v[word] = vector\n",
    "        else:\n",
    "            print(\"There was an issue with \" + word)\n",
    "        # let's check the vocabulary size\n",
    "print(\"Dictionary Size: \", len(dict_w2v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "596fa2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 50\n",
    "embedding_matrix = np.zeros((len(word2idx), embedding_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e84579c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unknown words:  28423\n"
     ]
    }
   ],
   "source": [
    "unk_cnt = 0\n",
    "unk_set = set()\n",
    "for word in word2idx.keys():\n",
    "    embedding_vector = dict_w2v.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        tkn_id = word2idx[word]\n",
    "        embedding_matrix[tkn_id-1] = embedding_vector\n",
    "    else:\n",
    "        unk_cnt += 1\n",
    "        unk_set.add(word)\n",
    "# Print how many weren't found\n",
    "print(\"Total unknown words: \", unk_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557d6b60",
   "metadata": {},
   "source": [
    "# Building Model With PreTrained Embedding Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a2c0021",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_bilstm(vocab_size, embedding_dim,rnn_units, batch_size, train_emb=False):\n",
    "    model = tf.keras.Sequential([\n",
    "    layers.Embedding(vocab_size, embedding_dim, mask_zero=True,weights=[embedding_matrix], trainable=train_emb),\n",
    "    layers.Bidirectional(layers.LSTM(rnn_units, return_sequences=True,dropout=0.5)),\n",
    "    layers.Bidirectional(layers.LSTM(rnn_units, dropout=0.25)),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721649d2",
   "metadata": {},
   "source": [
    "# Feature extraction as we set the weights of embedding Fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5cfa8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 50)          4429100   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, None, 128)         58880     \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 4,586,925\n",
      "Trainable params: 157,825\n",
      "Non-trainable params: 4,429,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_fe = build_model_bilstm(vocab_size = vocab_size,embedding_dim=embedding_dim,rnn_units=rnn_units,batch_size=BATCH_SIZE)\n",
    "model_fe.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36aab049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "250/250 [==============================] - 26s 53ms/step - loss: 0.6887 - accuracy: 0.5350 - precision: 0.5363 - recall: 0.5176\n",
      "Epoch 2/10\n",
      "250/250 [==============================] - 13s 52ms/step - loss: 0.6759 - accuracy: 0.5742 - precision: 0.5779 - recall: 0.5506\n",
      "Epoch 3/10\n",
      "250/250 [==============================] - 13s 53ms/step - loss: 0.6765 - accuracy: 0.5689 - precision: 0.5741 - recall: 0.5337\n",
      "Epoch 4/10\n",
      "250/250 [==============================] - 14s 54ms/step - loss: 0.6740 - accuracy: 0.5696 - precision: 0.5731 - recall: 0.5462\n",
      "Epoch 5/10\n",
      "250/250 [==============================] - 13s 53ms/step - loss: 0.6552 - accuracy: 0.6061 - precision: 0.6094 - recall: 0.5912\n",
      "Epoch 6/10\n",
      "250/250 [==============================] - 13s 53ms/step - loss: 0.6376 - accuracy: 0.6318 - precision: 0.6358 - recall: 0.6170\n",
      "Epoch 7/10\n",
      "250/250 [==============================] - 13s 53ms/step - loss: 0.6266 - accuracy: 0.6438 - precision: 0.6444 - recall: 0.6416\n",
      "Epoch 8/10\n",
      "250/250 [==============================] - 13s 53ms/step - loss: 0.6133 - accuracy: 0.6570 - precision: 0.6562 - recall: 0.6595\n",
      "Epoch 9/10\n",
      "250/250 [==============================] - 13s 53ms/step - loss: 0.6123 - accuracy: 0.6580 - precision: 0.6573 - recall: 0.6602\n",
      "Epoch 10/10\n",
      "250/250 [==============================] - 13s 53ms/step - loss: 0.5916 - accuracy: 0.6778 - precision: 0.6795 - recall: 0.6730\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c1e0eae460>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fe.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy', 'Precision', 'Recall'])\n",
    "\n",
    "model_fe.fit(encoded_train_batched, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ba59cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 5s 21ms/step - loss: 0.5682 - accuracy: 0.6933 - precision: 0.7870 - recall: 0.5301\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5681567788124084,\n",
       " 0.6933199763298035,\n",
       " 0.7870293259620667,\n",
       " 0.5300800204277039]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fe.evaluate(test_data.batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b708d24c",
   "metadata": {},
   "source": [
    "we can see that having fixed embedding layer make our model worse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c20c116",
   "metadata": {},
   "source": [
    "# Fine-tuning model as we fine tune Embedding layer weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b781c43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 50)          4429100   \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, None, 128)         58880     \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 128)               98816     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 4,586,925\n",
      "Trainable params: 4,586,925\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_ft = build_model_bilstm(vocab_size=vocab_size,embedding_dim=embedding_dim,rnn_units=rnn_units,batch_size=BATCH_SIZE,train_emb=True)\n",
    "model_ft.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "037a47af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "250/250 [==============================] - 28s 56ms/step - loss: 0.6681 - accuracy: 0.5696 - precision: 0.5704 - recall: 0.5638\n",
      "Epoch 2/10\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 0.4819 - accuracy: 0.7672 - precision: 0.7647 - recall: 0.7719\n",
      "Epoch 3/10\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 0.3521 - accuracy: 0.8457 - precision: 0.8444 - recall: 0.8477\n",
      "Epoch 4/10\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 0.2956 - accuracy: 0.8778 - precision: 0.8765 - recall: 0.8795\n",
      "Epoch 5/10\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 0.2332 - accuracy: 0.9074 - precision: 0.9079 - recall: 0.9066\n",
      "Epoch 6/10\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 0.2003 - accuracy: 0.9192 - precision: 0.9204 - recall: 0.9178\n",
      "Epoch 7/10\n",
      "250/250 [==============================] - 14s 56ms/step - loss: 0.1633 - accuracy: 0.9373 - precision: 0.9362 - recall: 0.9385\n",
      "Epoch 8/10\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 0.1400 - accuracy: 0.9472 - precision: 0.9486 - recall: 0.9457\n",
      "Epoch 9/10\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 0.1082 - accuracy: 0.9602 - precision: 0.9604 - recall: 0.9601\n",
      "Epoch 10/10\n",
      "250/250 [==============================] - 14s 55ms/step - loss: 0.0942 - accuracy: 0.9668 - precision: 0.9684 - recall: 0.9650\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c2b2ee7eb0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy', 'Precision', 'Recall'])\n",
    "model_ft.fit(encoded_train_batched, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "206d5fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 9s 22ms/step - loss: 0.5539 - accuracy: 0.8525 - precision: 0.8373 - recall: 0.8750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5538679957389832,\n",
       " 0.8525199890136719,\n",
       " 0.8373268246650696,\n",
       " 0.8750399947166443]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft.evaluate(test_data.batch(BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2176b445",
   "metadata": {},
   "source": [
    "as we can see their are great improvment when we fine tune the embedding layer but the model tends to overfit we can solve this by adding more dropout layers between embeding layer and lstm layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792e2203",
   "metadata": {},
   "source": [
    "For now we had a fixed vector for each word that is known as Static Word Embedding\n",
    "\n",
    "Next we will look at a Dynamic word Embedding in which we will use BERT Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dbb334",
   "metadata": {},
   "source": [
    "# Using Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dead73cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9d98edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 208k/208k [00:00<00:00, 511kB/s] \n",
      "Downloading: 100%|██████████| 29.0/29.0 [00:00<00:00, 14.5kB/s]\n",
      "Downloading: 100%|██████████| 570/570 [00:00<00:00, 569kB/s]\n"
     ]
    }
   ],
   "source": [
    "bert_name = 'bert-base-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_name,\n",
    "                                          add_special_tokens=True,\n",
    "                                          do_lower_case=False,\n",
    "                                          max_length=150,\n",
    "                                          pad_to_max_length=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ee475c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_encoder(review):\n",
    "    txt = review.numpy().decode('utf-8')\n",
    "    \n",
    "    encoded = tokenizer.encode_plus(txt, add_special_tokens=True,\n",
    "    max_length=150,\n",
    "    pad_to_max_length=True,\n",
    "    return_attention_mask=True,\n",
    "    return_token_type_ids=True)\n",
    "    \n",
    "    return encoded['input_ids'], encoded['token_type_ids'], encoded['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc8f7e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\Osama Ismael\\Anaconda3\\envs\\env1\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2285: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "bert_train = [bert_encoder(r) for r, l in imdb_train]\n",
    "bert_lbl = [l for r, l in imdb_train]\n",
    "bert_train = np.array(bert_train)\n",
    "bert_lbl = tf.keras.utils.to_categorical(bert_lbl, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce1e275f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 3, 150) (20000, 2)\n"
     ]
    }
   ],
   "source": [
    "# create training and validation splits\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(bert_train,bert_lbl,test_size=0.2,random_state=42)\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3991709",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_reviews, tr_segments, tr_masks = np.split(x_train, 3, axis=1)\n",
    "val_reviews, val_segments, val_masks = np.split(x_val, 3, axis=1)\n",
    "\n",
    "tr_reviews = tr_reviews.squeeze()\n",
    "tr_segments = tr_segments.squeeze()\n",
    "tr_masks = tr_masks.squeeze()\n",
    "\n",
    "val_reviews = val_reviews.squeeze()\n",
    "val_segments = val_segments.squeeze()\n",
    "val_masks = val_masks.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "901236de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_to_features(input_ids,attention_masks,token_type_ids,y):\n",
    "    return {\"input_ids\": input_ids,\"attention_mask\": attention_masks,\n",
    "            \"token_type_ids\": token_type_ids}, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ef24c8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((tr_reviews,tr_masks, tr_segments, y_train)). \\\n",
    "                                                map(example_to_features).shuffle(100).batch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d1652aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ds = tf.data.Dataset.from_tensor_slices((val_reviews,val_masks, val_segments, y_val)).\\\n",
    "                                                map(example_to_features).shuffle(100).batch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "97dc15f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 502M/502M [02:30<00:00, 3.50MB/s] \n",
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained(bert_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9fdf2cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  108310272 \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  1538      \n",
      "=================================================================\n",
      "Total params: 108,311,810\n",
      "Trainable params: 108,311,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "bert_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "bert_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9e37a964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning BERT on IMDB\n",
      "Epoch 1/3\n",
      "2500/2500 [==============================] - 736s 289ms/step - loss: 0.3522 - accuracy: 0.8456 - val_loss: 0.2709 - val_accuracy: 0.8868\n",
      "Epoch 2/3\n",
      "2500/2500 [==============================] - 715s 286ms/step - loss: 0.1929 - accuracy: 0.9270 - val_loss: 0.3319 - val_accuracy: 0.8818\n",
      "Epoch 3/3\n",
      "2500/2500 [==============================] - 711s 284ms/step - loss: 0.0987 - accuracy: 0.9676 - val_loss: 0.3113 - val_accuracy: 0.8864\n"
     ]
    }
   ],
   "source": [
    "print(\"Fine-tuning BERT on IMDB\")\n",
    "bert_history = bert_model.fit(train_ds, epochs=3,validation_data=valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c4ed1421",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Osama Ismael\\Anaconda3\\envs\\env1\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2285: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# prep data for testing\n",
    "bert_test = [bert_encoder(r) for r,l in imdb_test]\n",
    "bert_tst_lbl = [l for r, l in imdb_test]\n",
    "bert_test2 = np.array(bert_test)\n",
    "bert_tst_lbl2 = tf.keras.utils.to_categorical (bert_tst_lbl,num_classes=2)\n",
    "\n",
    "ts_reviews, ts_segments, ts_masks = np.split(bert_test2, 3, axis=1)\n",
    "\n",
    "ts_reviews = ts_reviews.squeeze()\n",
    "ts_segments = ts_segments.squeeze()\n",
    "ts_masks = ts_masks.squeeze()\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((ts_reviews,ts_masks, ts_segments, bert_tst_lbl2)).\\\n",
    "                                                map(example_to_features).shuffle(100).batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "64b6aba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 251s 160ms/step - loss: 0.3387 - accuracy: 0.8784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3387053608894348, 0.8784000277519226]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e64b37",
   "metadata": {},
   "source": [
    "we can see That Bert has outperformed seq2seq model with only 3 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca99113c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
